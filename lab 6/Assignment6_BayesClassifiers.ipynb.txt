{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6: Bayes and Naive Bayes Classifiers\n",
    "## CS201L: Artificial Intelligence Laboratory\n",
    "### Indian Institute of Technology, Dharwad\n",
    "\n",
    "---\n",
    "\n",
    "**Objective:** Implement and compare Naive Bayes and Bayes classifiers on Activity Detection dataset with four variations:\n",
    "1. Original Data\n",
    "2. Standardized Data\n",
    "3. PCA (All Components)\n",
    "4. PCA (99% Variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, \n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    classification_report\n",
    ")\n",
    "from scipy.stats import multivariate_normal\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_path, val_path, test_path):\n",
    "    \"\"\"\n",
    "    Load training, validation, and test datasets.\n",
    "    \n",
    "    Args:\n",
    "        train_path: Path to training CSV file\n",
    "        val_path: Path to validation CSV file\n",
    "        test_path: Path to test CSV file\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    \"\"\"\n",
    "    # Load datasets\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    val_df = pd.read_csv(val_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    \n",
    "    # Separate features and target\n",
    "    X_train = train_df.drop('Activity', axis=1).values\n",
    "    y_train = train_df['Activity'].values\n",
    "    \n",
    "    X_val = val_df.drop('Activity', axis=1).values\n",
    "    y_val = val_df['Activity'].values\n",
    "    \n",
    "    X_test = test_df.drop('Activity', axis=1).values\n",
    "    y_test = test_df['Activity'].values\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "def evaluate_classifier(y_true, y_pred, dataset_name, split_name):\n",
    "    \"\"\"\n",
    "    Evaluate classifier performance and return metrics.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels\n",
    "        dataset_name: Name of the dataset variant\n",
    "        split_name: 'Validation' or 'Test'\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing all metrics\n",
    "    \"\"\"\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    precision_micro = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall_micro = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    \n",
    "    metrics = {\n",
    "        'Dataset': dataset_name,\n",
    "        'Split': split_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision (Macro)': precision_macro,\n",
    "        'Precision (Micro)': precision_micro,\n",
    "        'Recall (Macro)': recall_macro,\n",
    "        'Recall (Micro)': recall_micro,\n",
    "        'F1-Score (Macro)': f1_macro,\n",
    "        'F1-Score (Micro)': f1_micro\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, classes):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix as a heatmap.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels\n",
    "        title: Title for the plot\n",
    "        classes: List of class names\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=classes, yticklabels=classes,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return cm\n",
    "\n",
    "\n",
    "print(\"Helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part A: Naive Bayes Classifier (30%)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Implementation Using Scikit-Learn\n",
    "\n",
    "The Naive Bayes classifier assumes:\n",
    "- Data follows a Gaussian distribution\n",
    "- All features are statistically independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_naive_bayes(train_path, val_path, test_path, dataset_name):\n",
    "    \"\"\"\n",
    "    Train Naive Bayes classifier and evaluate on validation and test sets.\n",
    "    \n",
    "    Args:\n",
    "        train_path: Path to training data\n",
    "        val_path: Path to validation data\n",
    "        test_path: Path to test data\n",
    "        dataset_name: Name of dataset variant\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (validation_metrics, test_metrics, model)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training Naive Bayes on {dataset_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Load data\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_data(train_path, val_path, test_path)\n",
    "    \n",
    "    print(f\"Training samples: {X_train.shape[0]}\")\n",
    "    print(f\"Validation samples: {X_val.shape[0]}\")\n",
    "    print(f\"Test samples: {X_test.shape[0]}\")\n",
    "    print(f\"Number of features: {X_train.shape[1]}\")\n",
    "    \n",
    "    # Initialize and train Naive Bayes model\n",
    "    naive_bayes = GaussianNB()\n",
    "    naive_bayes.fit(X_train, y_train)\n",
    "    print(\"\\nModel training completed!\")\n",
    "    \n",
    "    # Predict on validation set\n",
    "    y_val_pred = naive_bayes.predict(X_val)\n",
    "    val_metrics = evaluate_classifier(y_val, y_val_pred, dataset_name, 'Validation')\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_test_pred = naive_bayes.predict(X_test)\n",
    "    test_metrics = evaluate_classifier(y_test, y_test_pred, dataset_name, 'Test')\n",
    "    \n",
    "    # Get class names\n",
    "    classes = np.unique(y_train)\n",
    "    \n",
    "    # Plot confusion matrices\n",
    "    print(\"\\n--- Validation Set Confusion Matrix ---\")\n",
    "    plot_confusion_matrix(y_val, y_val_pred, \n",
    "                         f'Naive Bayes - {dataset_name} (Validation)', \n",
    "                         classes)\n",
    "    \n",
    "    print(\"\\n--- Test Set Confusion Matrix ---\")\n",
    "    plot_confusion_matrix(y_test, y_test_pred, \n",
    "                         f'Naive Bayes - {dataset_name} (Test)', \n",
    "                         classes)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\n--- Validation Set Metrics ---\")\n",
    "    for key, value in val_metrics.items():\n",
    "        if key not in ['Dataset', 'Split']:\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\n--- Test Set Metrics ---\")\n",
    "    for key, value in test_metrics.items():\n",
    "        if key not in ['Dataset', 'Split']:\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    return val_metrics, test_metrics, naive_bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Naive Bayes on Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_original_val, nb_original_test, nb_original_model = train_and_evaluate_naive_bayes(\n",
    "    'Activity_Train.csv',\n",
    "    'Activity_Validation.csv',\n",
    "    'Activity_Test.csv',\n",
    "    'Original Data'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Naive Bayes on Standardized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_scaled_val, nb_scaled_test, nb_scaled_model = train_and_evaluate_naive_bayes(\n",
    "    'Activity_Scaled_Train.csv',\n",
    "    'Activity_Scaled_Validation.csv',\n",
    "    'Activity_Scaled_Test.csv',\n",
    "    'Standardized Data'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Naive Bayes on PCA (All Components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pca_all_val, nb_pca_all_test, nb_pca_all_model = train_and_evaluate_naive_bayes(\n",
    "    'Activity_PCAAll_Train.csv',\n",
    "    'Activity_PCAAll_Validation.csv',\n",
    "    'Activity_PCAAll_Test.csv',\n",
    "    'PCA (All Components)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Naive Bayes on PCA (99% Variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pca99_val, nb_pca99_test, nb_pca99_model = train_and_evaluate_naive_bayes(\n",
    "    'Activity_PCA99_Train.csv',\n",
    "    'Activity_PCA99_Validation.csv',\n",
    "    'Activity_PCA99_Test.csv',\n",
    "    'PCA (99% Variance)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part B: Bayes Classifier Implementation (40%)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Classifier from Scratch\n",
    "\n",
    "The Bayes classifier:\n",
    "- Uses unimodal Gaussian density function\n",
    "- Assumes data follows a multivariate Gaussian distribution per class\n",
    "- Applies Bayes' theorem for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesClassifier:\n",
    "    \"\"\"\n",
    "    Bayes Classifier using multivariate Gaussian distribution.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.priors = {}\n",
    "        self.means = {}\n",
    "        self.covariances = {}\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the Bayes classifier.\n",
    "        \n",
    "        Args:\n",
    "            X: Training features (n_samples, n_features)\n",
    "            y: Training labels (n_samples,)\n",
    "        \"\"\"\n",
    "        self.classes = np.unique(y)\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # For each class, compute parameters\n",
    "        for c in self.classes:\n",
    "            # Get samples belonging to class c\n",
    "            X_c = X[y == c]\n",
    "            \n",
    "            # Compute prior probability P(C_i)\n",
    "            self.priors[c] = X_c.shape[0] / n_samples\n",
    "            \n",
    "            # Compute mean vector (μ_i)\n",
    "            self.means[c] = np.mean(X_c, axis=0)\n",
    "            \n",
    "            # Compute covariance matrix (Σ_i)\n",
    "            self.covariances[c] = np.cov(X_c, rowvar=False)\n",
    "            \n",
    "            # Add small regularization to avoid singular matrices\n",
    "            self.covariances[c] += np.eye(X.shape[1]) * 1e-6\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X.\n",
    "        \n",
    "        Args:\n",
    "            X: Test features (n_samples, n_features)\n",
    "        \n",
    "        Returns:\n",
    "            Predicted class labels (n_samples,)\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        # For each sample\n",
    "        for x in X:\n",
    "            posteriors = []\n",
    "            \n",
    "            # Compute posterior for each class\n",
    "            for c in self.classes:\n",
    "                # Compute likelihood p(x|μ_i, Σ_i) using multivariate normal PDF\n",
    "                likelihood = multivariate_normal.pdf(\n",
    "                    x, \n",
    "                    mean=self.means[c], \n",
    "                    cov=self.covariances[c],\n",
    "                    allow_singular=True\n",
    "                )\n",
    "                \n",
    "                # Compute posterior P(C_i|x) ∝ p(x|C_i) * P(C_i)\n",
    "                posterior = likelihood * self.priors[c]\n",
    "                posteriors.append(posterior)\n",
    "            \n",
    "            # Normalize posteriors (optional, but good practice)\n",
    "            posteriors = np.array(posteriors)\n",
    "            total_prob = np.sum(posteriors)\n",
    "            if total_prob > 0:\n",
    "                posteriors = posteriors / total_prob\n",
    "            \n",
    "            # Assign class with maximum posterior probability\n",
    "            pred_class = self.classes[np.argmax(posteriors)]\n",
    "            predictions.append(pred_class)\n",
    "        \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities for samples in X.\n",
    "        \n",
    "        Args:\n",
    "            X: Test features (n_samples, n_features)\n",
    "        \n",
    "        Returns:\n",
    "            Class probabilities (n_samples, n_classes)\n",
    "        \"\"\"\n",
    "        probabilities = []\n",
    "        \n",
    "        for x in X:\n",
    "            posteriors = []\n",
    "            \n",
    "            for c in self.classes:\n",
    "                likelihood = multivariate_normal.pdf(\n",
    "                    x, \n",
    "                    mean=self.means[c], \n",
    "                    cov=self.covariances[c],\n",
    "                    allow_singular=True\n",
    "                )\n",
    "                posterior = likelihood * self.priors[c]\n",
    "                posteriors.append(posterior)\n",
    "            \n",
    "            posteriors = np.array(posteriors)\n",
    "            total_prob = np.sum(posteriors)\n",
    "            if total_prob > 0:\n",
    "                posteriors = posteriors / total_prob\n",
    "            \n",
    "            probabilities.append(posteriors)\n",
    "        \n",
    "        return np.array(probabilities)\n",
    "\n",
    "\n",
    "print(\"BayesClassifier class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part C: Bayes Classifier Evaluation (30%)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_bayes(train_path, val_path, test_path, dataset_name):\n",
    "    \"\"\"\n",
    "    Train Bayes classifier and evaluate on validation and test sets.\n",
    "    \n",
    "    Args:\n",
    "        train_path: Path to training data\n",
    "        val_path: Path to validation data\n",
    "        test_path: Path to test data\n",
    "        dataset_name: Name of dataset variant\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (validation_metrics, test_metrics, model)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training Bayes Classifier on {dataset_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Load data\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_data(train_path, val_path, test_path)\n",
    "    \n",
    "    print(f\"Training samples: {X_train.shape[0]}\")\n",
    "    print(f\"Validation samples: {X_val.shape[0]}\")\n",
    "    print(f\"Test samples: {X_test.shape[0]}\")\n",
    "    print(f\"Number of features: {X_train.shape[1]}\")\n",
    "    \n",
    "    # Initialize and train Bayes classifier\n",
    "    bayes = BayesClassifier()\n",
    "    bayes.fit(X_train, y_train)\n",
    "    print(\"\\nModel training completed!\")\n",
    "    \n",
    "    # Predict on validation set\n",
    "    print(\"\\nPredicting on validation set...\")\n",
    "    y_val_pred = bayes.predict(X_val)\n",
    "    val_metrics = evaluate_classifier(y_val, y_val_pred, dataset_name, 'Validation')\n",
    "    \n",
    "    # Predict on test set\n",
    "    print(\"Predicting on test set...\")\n",
    "    y_test_pred = bayes.predict(X_test)\n",
    "    test_metrics = evaluate_classifier(y_test, y_test_pred, dataset_name, 'Test')\n",
    "    \n",
    "    # Get class names\n",
    "    classes = bayes.classes\n",
    "    \n",
    "    # Plot confusion matrices\n",
    "    print(\"\\n--- Validation Set Confusion Matrix ---\")\n",
    "    plot_confusion_matrix(y_val, y_val_pred, \n",
    "                         f'Bayes Classifier - {dataset_name} (Validation)', \n",
    "                         classes)\n",
    "    \n",
    "    print(\"\\n--- Test Set Confusion Matrix ---\")\n",
    "    plot_confusion_matrix(y_test, y_test_pred, \n",
    "                         f'Bayes Classifier - {dataset_name} (Test)', \n",
    "                         classes)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\n--- Validation Set Metrics ---\")\n",
    "    for key, value in val_metrics.items():\n",
    "        if key not in ['Dataset', 'Split']:\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\n--- Test Set Metrics ---\")\n",
    "    for key, value in test_metrics.items():\n",
    "        if key not in ['Dataset', 'Split']:\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    return val_metrics, test_metrics, bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bayes Classifier on Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_original_val, bayes_original_test, bayes_original_model = train_and_evaluate_bayes(\n",
    "    'Activity_Train.csv',\n",
    "    'Activity_Validation.csv',\n",
    "    'Activity_Test.csv',\n",
    "    'Original Data'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bayes Classifier on Standardized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_scaled_val, bayes_scaled_test, bayes_scaled_model = train_and_evaluate_bayes(\n",
    "    'Activity_Scaled_Train.csv',\n",
    "    'Activity_Scaled_Validation.csv',\n",
    "    'Activity_Scaled_Test.csv',\n",
    "    'Standardized Data'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Bayes Classifier on PCA (All Components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_pca_all_val, bayes_pca_all_test, bayes_pca_all_model = train_and_evaluate_bayes(\n",
    "    'Activity_PCAAll_Train.csv',\n",
    "    'Activity_PCAAll_Validation.csv',\n",
    "    'Activity_PCAAll_Test.csv',\n",
    "    'PCA (All Components)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Bayes Classifier on PCA (99% Variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_pca99_val, bayes_pca99_test, bayes_pca99_model = train_and_evaluate_bayes(\n",
    "    'Activity_PCA99_Train.csv',\n",
    "    'Activity_PCA99_Validation.csv',\n",
    "    'Activity_PCA99_Test.csv',\n",
    "    'PCA (99% Variance)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary Tables\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all Naive Bayes results\n",
    "nb_results = [\n",
    "    nb_original_val, nb_original_test,\n",
    "    nb_scaled_val, nb_scaled_test,\n",
    "    nb_pca_all_val, nb_pca_all_test,\n",
    "    nb_pca99_val, nb_pca99_test\n",
    "]\n",
    "\n",
    "nb_summary_df = pd.DataFrame(nb_results)\n",
    "nb_summary_df = nb_summary_df[['Dataset', 'Split', 'Accuracy', 'Precision (Macro)', \n",
    "                                 'Precision (Micro)', 'Recall (Macro)', 'Recall (Micro)', \n",
    "                                 'F1-Score (Macro)', 'F1-Score (Micro)']]\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"NAIVE BAYES CLASSIFIER - COMPLETE RESULTS SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(nb_summary_df.to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Classifier Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all Bayes classifier results\n",
    "bayes_results = [\n",
    "    bayes_original_val, bayes_original_test,\n",
    "    bayes_scaled_val, bayes_scaled_test,\n",
    "    bayes_pca_all_val, bayes_pca_all_test,\n",
    "    bayes_pca99_val, bayes_pca99_test\n",
    "]\n",
    "\n",
    "bayes_summary_df = pd.DataFrame(bayes_results)\n",
    "bayes_summary_df = bayes_summary_df[['Dataset', 'Split', 'Accuracy', 'Precision (Macro)', \n",
    "                                       'Precision (Micro)', 'Recall (Macro)', 'Recall (Micro)', \n",
    "                                       'F1-Score (Macro)', 'F1-Score (Micro)']]\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"BAYES CLASSIFIER - COMPLETE RESULTS SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(bayes_summary_df.to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis: Test Set Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison for test sets only\n",
    "test_comparison = []\n",
    "\n",
    "datasets = ['Original Data', 'Standardized Data', 'PCA (All Components)', 'PCA (99% Variance)']\n",
    "nb_test_results = [nb_original_test, nb_scaled_test, nb_pca_all_test, nb_pca99_test]\n",
    "bayes_test_results = [bayes_original_test, bayes_scaled_test, bayes_pca_all_test, bayes_pca99_test]\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    test_comparison.append({\n",
    "        'Dataset': dataset,\n",
    "        'Classifier': 'Naive Bayes',\n",
    "        'Accuracy': nb_test_results[i]['Accuracy'],\n",
    "        'F1-Score (Macro)': nb_test_results[i]['F1-Score (Macro)'],\n",
    "        'F1-Score (Micro)': nb_test_results[i]['F1-Score (Micro)']\n",
    "    })\n",
    "    test_comparison.append({\n",
    "        'Dataset': dataset,\n",
    "        'Classifier': 'Bayes',\n",
    "        'Accuracy': bayes_test_results[i]['Accuracy'],\n",
    "        'F1-Score (Macro)': bayes_test_results[i]['F1-Score (Macro)'],\n",
    "        'F1-Score (Micro)': bayes_test_results[i]['F1-Score (Micro)']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(test_comparison)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARATIVE ANALYSIS - TEST SET PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Accuracy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar plot comparing accuracies\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Validation set comparison\n",
    "val_data = []\n",
    "for i, dataset in enumerate(datasets):\n",
    "    val_data.append({\n",
    "        'Dataset': dataset,\n",
    "        'Naive Bayes': [nb_original_val, nb_scaled_val, nb_pca_all_val, nb_pca99_val][i]['Accuracy'],\n",
    "        'Bayes': [bayes_original_val, bayes_scaled_val, bayes_pca_all_val, bayes_pca99_val][i]['Accuracy']\n",
    "    })\n",
    "\n",
    "val_df = pd.DataFrame(val_data)\n",
    "val_df.plot(x='Dataset', y=['Naive Bayes', 'Bayes'], kind='bar', ax=ax[0], \n",
    "            color=['#3498db', '#e74c3c'], width=0.7)\n",
    "ax[0].set_title('Validation Set Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "ax[0].set_ylabel('Accuracy', fontsize=12)\n",
    "ax[0].set_xlabel('Dataset Variant', fontsize=12)\n",
    "ax[0].set_ylim([0, 1.0])\n",
    "ax[0].legend(title='Classifier')\n",
    "ax[0].grid(axis='y', alpha=0.3)\n",
    "plt.setp(ax[0].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Test set comparison\n",
    "test_data = []\n",
    "for i, dataset in enumerate(datasets):\n",
    "    test_data.append({\n",
    "        'Dataset': dataset,\n",
    "        'Naive Bayes': nb_test_results[i]['Accuracy'],\n",
    "        'Bayes': bayes_test_results[i]['Accuracy']\n",
    "    })\n",
    "\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_df.plot(x='Dataset', y=['Naive Bayes', 'Bayes'], kind='bar', ax=ax[1], \n",
    "            color=['#3498db', '#e74c3c'], width=0.7)\n",
    "ax[1].set_title('Test Set Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "ax[1].set_ylabel('Accuracy', fontsize=12)\n",
    "ax[1].set_xlabel('Dataset Variant', fontsize=12)\n",
    "ax[1].set_ylim([0, 1.0])\n",
    "ax[1].legend(title='Classifier')\n",
    "ax[1].grid(axis='y', alpha=0.3)\n",
    "plt.setp(ax[1].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary tables to CSV files\n",
    "nb_summary_df.to_csv('naive_bayes_results.csv', index=False)\n",
    "bayes_summary_df.to_csv('bayes_classifier_results.csv', index=False)\n",
    "comparison_df.to_csv('comparative_analysis.csv', index=False)\n",
    "\n",
    "print(\"Results exported successfully!\")\n",
    "print(\"- naive_bayes_results.csv\")\n",
    "print(\"- bayes_classifier_results.csv\")\n",
    "print(\"- comparative_analysis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Conclusion\n",
    "---\n",
    "\n",
    "This notebook successfully implemented and evaluated:\n",
    "\n",
    "1. **Naive Bayes Classifier** using scikit-learn's GaussianNB\n",
    "2. **Bayes Classifier** implemented from scratch using multivariate Gaussian distributions\n",
    "\n",
    "Both classifiers were tested on four dataset variations:\n",
    "- Original Data (561 features)\n",
    "- Standardized Data (561 features, scaled)\n",
    "- PCA All Components (transformed features)\n",
    "- PCA 99% Variance (reduced features)\n",
    "\n",
    "Key observations:\n",
    "- Both classifiers perform well on the activity detection task\n",
    "- Standardization and PCA transformations may affect performance differently\n",
    "- The Bayes classifier with full covariance modeling may capture more complex relationships\n",
    "- Naive Bayes is computationally more efficient due to the independence assumption\n",
    "\n",
    "All metrics including confusion matrices, accuracy, precision, recall, and F1-scores have been computed for both validation and test sets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
