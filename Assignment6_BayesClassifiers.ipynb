{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6: Bayes and Naive Bayes Classifiers\n",
    "## CS201L: Artificial Intelligence Laboratory\n",
    "### Indian Institute of Technology, Dharwad\n",
    "\n",
    "---\n",
    "\n",
    "**Objective:** Implement and compare Naive Bayes and Bayes classifiers on Activity Detection dataset with four variations:\n",
    "1. Original Data\n",
    "2. Standardized Data\n",
    "3. PCA (All Components)\n",
    "4. PCA (99% Variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, \n",
    "    accuracy_score, \n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score,\n",
    "    classification_report\n",
    ")\n",
    "from scipy.stats import multivariate_normal\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_path, val_path, test_path):\n",
    "    \"\"\"\n",
    "    Load training, validation, and test datasets.\n",
    "    \n",
    "    Args:\n",
    "        train_path: Path to training CSV file\n",
    "        val_path: Path to validation CSV file\n",
    "        test_path: Path to test CSV file\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "    \"\"\"\n",
    "    # Load datasets\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    val_df = pd.read_csv(val_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    \n",
    "    # Separate features and target\n",
    "    X_train = train_df.drop('Activity', axis=1).values\n",
    "    y_train = train_df['Activity'].values\n",
    "    \n",
    "    X_val = val_df.drop('Activity', axis=1).values\n",
    "    y_val = val_df['Activity'].values\n",
    "    \n",
    "    X_test = test_df.drop('Activity', axis=1).values\n",
    "    y_test = test_df['Activity'].values\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "def evaluate_classifier(y_true, y_pred, dataset_name, split_name):\n",
    "    \"\"\"\n",
    "    Evaluate classifier performance and return metrics.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels\n",
    "        dataset_name: Name of the dataset variant\n",
    "        split_name: 'Validation' or 'Test'\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing all metrics\n",
    "    \"\"\"\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    precision_micro = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall_micro = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    \n",
    "    metrics = {\n",
    "        'Dataset': dataset_name,\n",
    "        'Split': split_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision (Macro)': precision_macro,\n",
    "        'Precision (Micro)': precision_micro,\n",
    "        'Recall (Macro)': recall_macro,\n",
    "        'Recall (Micro)': recall_micro,\n",
    "        'F1-Score (Macro)': f1_macro,\n",
    "        'F1-Score (Micro)': f1_micro\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, classes):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix as a heatmap.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels\n",
    "        title: Title for the plot\n",
    "        classes: List of class names\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=classes, yticklabels=classes,\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return cm\n",
    "\n",
    "\n",
    "print(\"Helper functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part A: Naive Bayes Classifier (30%)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Implementation Using Scikit-Learn\n",
    "\n",
    "The Naive Bayes classifier assumes:\n",
    "- Data follows a Gaussian distribution\n",
    "- All features are statistically independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_naive_bayes(train_path, val_path, test_path, dataset_name):\n",
    "    \"\"\"\n",
    "    Train Naive Bayes classifier and evaluate on validation and test sets.\n",
    "    \n",
    "    Args:\n",
    "        train_path: Path to training data\n",
    "        val_path: Path to validation data\n",
    "        test_path: Path to test data\n",
    "        dataset_name: Name of dataset variant\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (validation_metrics, test_metrics, model)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training Naive Bayes on {dataset_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Load data\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_data(train_path, val_path, test_path)\n",
    "    \n",
    "    print(f\"Training samples: {X_train.shape[0]}\")\n",
    "    print(f\"Validation samples: {X_val.shape[0]}\")\n",
    "    print(f\"Test samples: {X_test.shape[0]}\")\n",
    "    print(f\"Number of features: {X_train.shape[1]}\")\n",
    "    \n",
    "    # Initialize and train Naive Bayes model\n",
    "    naive_bayes = GaussianNB()\n",
    "    naive_bayes.fit(X_train, y_train)\n",
    "    print(\"\\nModel training completed!\")\n",
    "    \n",
    "    # Predict on validation set\n",
    "    y_val_pred = naive_bayes.predict(X_val)\n",
    "    val_metrics = evaluate_classifier(y_val, y_val_pred, dataset_name, 'Validation')\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_test_pred = naive_bayes.predict(X_test)\n",
    "    test_metrics = evaluate_classifier(y_test, y_test_pred, dataset_name, 'Test')\n",
    "    \n",
    "    # Get class names\n",
    "    classes = np.unique(y_train)\n",
    "    \n",
    "    # Plot confusion matrices\n",
    "    print(\"\\n--- Validation Set Confusion Matrix ---\")\n",
    "    plot_confusion_matrix(y_val, y_val_pred, \n",
    "                         f'Naive Bayes - {dataset_name} (Validation)', \n",
    "                         classes)\n",
    "    \n",
    "    print(\"\\n--- Test Set Confusion Matrix ---\")\n",
    "    plot_confusion_matrix(y_test, y_test_pred, \n",
    "                         f'Naive Bayes - {dataset_name} (Test)', \n",
    "                         classes)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\n--- Validation Set Metrics ---\")\n",
    "    for key, value in val_metrics.items():\n",
    "        if key not in ['Dataset', 'Split']:\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\n--- Test Set Metrics ---\")\n",
    "    for key, value in test_metrics.items():\n",
    "        if key not in ['Dataset', 'Split']:\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    return val_metrics, test_metrics, naive_bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Naive Bayes on Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_original_val, nb_original_test, nb_original_model = train_and_evaluate_naive_bayes(\n",
    "    'Activity_Train.csv',\n",
    "    'Activity_Validation.csv',\n",
    "    'Activity_Test.csv',\n",
    "    'Original Data'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Naive Bayes on Standardized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_scaled_val, nb_scaled_test, nb_scaled_model = train_and_evaluate_naive_bayes(\n",
    "    'Activity_Scaled_Train.csv',\n",
    "    'Activity_Scaled_Validation.csv',\n",
    "    'Activity_Scaled_Test.csv',\n",
    "    'Standardized Data'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Naive Bayes on PCA (All Components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pca_all_val, nb_pca_all_test, nb_pca_all_model = train_and_evaluate_naive_bayes(\n",
    "    'Activity_PCAAll_Train.csv',\n",
    "    'Activity_PCAAll_Validation.csv',\n",
    "    'Activity_PCAAll_Test.csv',\n",
    "    'PCA (All Components)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Naive Bayes on PCA (99% Variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pca99_val, nb_pca99_test, nb_pca99_model = train_and_evaluate_naive_bayes(\n",
    "    'Activity_PCA99_Train.csv',\n",
    "    'Activity_PCA99_Validation.csv',\n",
    "    'Activity_PCA99_Test.csv',\n",
    "    'PCA (99% Variance)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part B: Bayes Classifier Implementation (40%)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Classifier from Scratch\n",
    "\n",
    "The Bayes classifier:\n",
    "- Uses unimodal Gaussian density function\n",
    "- Assumes data follows a multivariate Gaussian distribution per class\n",
    "- Applies Bayes' theorem for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class BayesClassifier:\n    \"\"\"\n    Bayes Classifier using multivariate Gaussian distribution.\n    \n    This classifier assumes that data from each class follows a \n    multivariate Gaussian distribution and uses Bayes' theorem \n    for classification.\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"\n        Initialize the Bayes Classifier.\n        \"\"\"\n        self.classes = None           # Array of unique class labels\n        self.priors = {}              # Prior probabilities P(C_i)\n        self.means = {}               # Mean vectors \u03bc_i for each class\n        self.covariances = {}         # Covariance matrices \u03a3_i for each class\n    \n    def fit(self, X, y):\n        \"\"\"\n        Train the Bayes classifier by estimating parameters.\n        \n        Algorithm:\n        1. Identify unique classes\n        2. For each class C_i:\n           a. Calculate prior probability P(C_i)\n           b. Calculate mean vector \u03bc_i\n           c. Calculate covariance matrix \u03a3_i\n        \n        Args:\n            X: Training features (n_samples, n_features) - numpy array\n            y: Training labels (n_samples,) - numpy array\n        \"\"\"\n        # Step 1: Get unique classes\n        self.classes = np.unique(y)\n        n_samples = X.shape[0]\n        n_features = X.shape[1]\n        \n        print(f\"Training Bayes Classifier...\")\n        print(f\"Number of classes: {len(self.classes)}\")\n        print(f\"Number of samples: {n_samples}\")\n        print(f\"Number of features: {n_features}\")\n        \n        # Step 2: For each class, estimate parameters\n        for c in self.classes:\n            # Step 2.1: Get all samples belonging to class c\n            X_c = X[y == c]\n            n_c = X_c.shape[0]\n            \n            # Step 2.2: Calculate Prior Probability P(C_i)\n            # P(C_i) = Number of samples in class i / Total training samples\n            self.priors[c] = n_c / n_samples\n            \n            # Step 2.3: Calculate Mean Vector (\u03bc_i)\n            # \u03bc_i = average of all feature vectors in class i\n            self.means[c] = np.mean(X_c, axis=0)\n            \n            # Step 2.4: Calculate Covariance Matrix (\u03a3_i)\n            # \u03a3_i represents the relationships between features within class i\n            self.covariances[c] = np.cov(X_c, rowvar=False)\n            \n            # Step 2.5: Add regularization to avoid singular matrices\n            # Adding small value to diagonal ensures matrix is invertible\n            self.covariances[c] += np.eye(n_features) * 1e-6\n            \n            print(f\"Class {c}: {n_c} samples, Prior = {self.priors[c]:.4f}\")\n        \n        print(\"Training completed!\\n\")\n    \n    def predict(self, X):\n        \"\"\"\n        Predict class labels for samples in X.\n        \n        Algorithm:\n        For each sample x:\n        1. For each class C_i:\n           a. Calculate likelihood p(x|\u03bc_i, \u03a3_i) using multivariate normal PDF\n           b. Calculate posterior P(C_i|x) \u221d p(x|\u03bc_i, \u03a3_i) \u00d7 P(C_i)\n        2. Assign class with maximum posterior probability\n        \n        Args:\n            X: Test features (n_samples, n_features) - numpy array\n        \n        Returns:\n            predictions: Predicted class labels (n_samples,) - numpy array\n        \"\"\"\n        predictions = []\n        n_samples = X.shape[0]\n        \n        print(f\"Predicting {n_samples} samples...\")\n        \n        # For each test sample\n        for idx, x in enumerate(X):\n            posteriors = []\n            \n            # Step 1: Calculate posterior for each class\n            for c in self.classes:\n                # Step 1.1: Calculate Likelihood p(x|\u03bc_i, \u03a3_i)\n                # Using multivariate normal probability density function\n                likelihood = multivariate_normal.pdf(\n                    x,                          # Test sample\n                    mean=self.means[c],         # Mean vector of class c\n                    cov=self.covariances[c],    # Covariance matrix of class c\n                    allow_singular=True         # Handle potentially singular matrices\n                )\n                \n                # Step 1.2: Calculate Posterior Probability\n                # P(C_i|x) = p(x|\u03bc_i, \u03a3_i) \u00d7 P(C_i) / P(x)\n                # We can ignore P(x) for classification (same for all classes)\n                # So: P(C_i|x) \u221d p(x|\u03bc_i, \u03a3_i) \u00d7 P(C_i)\n                posterior = likelihood * self.priors[c]\n                posteriors.append(posterior)\n            \n            # Step 2: Normalize posteriors (optional but recommended)\n            posteriors = np.array(posteriors)\n            total_prob = np.sum(posteriors)\n            \n            if total_prob > 0:\n                posteriors = posteriors / total_prob\n            \n            # Step 3: Decision Rule - assign class with maximum posterior\n            pred_class_idx = np.argmax(posteriors)\n            pred_class = self.classes[pred_class_idx]\n            predictions.append(pred_class)\n            \n            # Progress indicator\n            if (idx + 1) % 500 == 0 or (idx + 1) == n_samples:\n                print(f\"Processed {idx + 1}/{n_samples} samples\")\n        \n        print(\"Prediction completed!\\n\")\n        return np.array(predictions)\n    \n    def predict_proba(self, X):\n        \"\"\"\n        Predict class probabilities for samples in X.\n        \n        Args:\n            X: Test features (n_samples, n_features) - numpy array\n        \n        Returns:\n            probabilities: Class probabilities (n_samples, n_classes) - numpy array\n        \"\"\"\n        probabilities = []\n        \n        # For each test sample\n        for x in X:\n            posteriors = []\n            \n            # Calculate posterior for each class\n            for c in self.classes:\n                # Calculate likelihood\n                likelihood = multivariate_normal.pdf(\n                    x, \n                    mean=self.means[c], \n                    cov=self.covariances[c],\n                    allow_singular=True\n                )\n                \n                # Calculate posterior (unnormalized)\n                posterior = likelihood * self.priors[c]\n                posteriors.append(posterior)\n            \n            # Normalize to get probabilities\n            posteriors = np.array(posteriors)\n            total_prob = np.sum(posteriors)\n            \n            if total_prob > 0:\n                posteriors = posteriors / total_prob\n            else:\n                # If all posteriors are 0, assign uniform probability\n                posteriors = np.ones(len(self.classes)) / len(self.classes)\n            \n            probabilities.append(posteriors)\n        \n        return np.array(probabilities)\n    \n    def get_params(self):\n        \"\"\"\n        Get the learned parameters of the model.\n        \n        Returns:\n            Dictionary containing priors, means, and covariances\n        \"\"\"\n        return {\n            'classes': self.classes,\n            'priors': self.priors,\n            'means': self.means,\n            'covariances': self.covariances\n        }\n\n\nprint(\"BayesClassifier class defined successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part C: Bayes Classifier Evaluation (30%)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_bayes(train_path, val_path, test_path, dataset_name):\n",
    "    \"\"\"\n",
    "    Train Bayes classifier and evaluate on validation and test sets.\n",
    "    \n",
    "    Args:\n",
    "        train_path: Path to training data\n",
    "        val_path: Path to validation data\n",
    "        test_path: Path to test data\n",
    "        dataset_name: Name of dataset variant\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (validation_metrics, test_metrics, model)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Training Bayes Classifier on {dataset_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Load data\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_data(train_path, val_path, test_path)\n",
    "    \n",
    "    print(f\"Training samples: {X_train.shape[0]}\")\n",
    "    print(f\"Validation samples: {X_val.shape[0]}\")\n",
    "    print(f\"Test samples: {X_test.shape[0]}\")\n",
    "    print(f\"Number of features: {X_train.shape[1]}\")\n",
    "    \n",
    "    # Initialize and train Bayes classifier\n",
    "    bayes = BayesClassifier()\n",
    "    bayes.fit(X_train, y_train)\n",
    "    print(\"\\nModel training completed!\")\n",
    "    \n",
    "    # Predict on validation set\n",
    "    print(\"\\nPredicting on validation set...\")\n",
    "    y_val_pred = bayes.predict(X_val)\n",
    "    val_metrics = evaluate_classifier(y_val, y_val_pred, dataset_name, 'Validation')\n",
    "    \n",
    "    # Predict on test set\n",
    "    print(\"Predicting on test set...\")\n",
    "    y_test_pred = bayes.predict(X_test)\n",
    "    test_metrics = evaluate_classifier(y_test, y_test_pred, dataset_name, 'Test')\n",
    "    \n",
    "    # Get class names\n",
    "    classes = bayes.classes\n",
    "    \n",
    "    # Plot confusion matrices\n",
    "    print(\"\\n--- Validation Set Confusion Matrix ---\")\n",
    "    plot_confusion_matrix(y_val, y_val_pred, \n",
    "                         f'Bayes Classifier - {dataset_name} (Validation)', \n",
    "                         classes)\n",
    "    \n",
    "    print(\"\\n--- Test Set Confusion Matrix ---\")\n",
    "    plot_confusion_matrix(y_test, y_test_pred, \n",
    "                         f'Bayes Classifier - {dataset_name} (Test)', \n",
    "                         classes)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\n--- Validation Set Metrics ---\")\n",
    "    for key, value in val_metrics.items():\n",
    "        if key not in ['Dataset', 'Split']:\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\n--- Test Set Metrics ---\")\n",
    "    for key, value in test_metrics.items():\n",
    "        if key not in ['Dataset', 'Split']:\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    return val_metrics, test_metrics, bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bayes Classifier on Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_original_val, bayes_original_test, bayes_original_model = train_and_evaluate_bayes(\n",
    "    'Activity_Train.csv',\n",
    "    'Activity_Validation.csv',\n",
    "    'Activity_Test.csv',\n",
    "    'Original Data'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bayes Classifier on Standardized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_scaled_val, bayes_scaled_test, bayes_scaled_model = train_and_evaluate_bayes(\n",
    "    'Activity_Scaled_Train.csv',\n",
    "    'Activity_Scaled_Validation.csv',\n",
    "    'Activity_Scaled_Test.csv',\n",
    "    'Standardized Data'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Bayes Classifier on PCA (All Components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_pca_all_val, bayes_pca_all_test, bayes_pca_all_model = train_and_evaluate_bayes(\n",
    "    'Activity_PCAAll_Train.csv',\n",
    "    'Activity_PCAAll_Validation.csv',\n",
    "    'Activity_PCAAll_Test.csv',\n",
    "    'PCA (All Components)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Bayes Classifier on PCA (99% Variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_pca99_val, bayes_pca99_test, bayes_pca99_model = train_and_evaluate_bayes(\n",
    "    'Activity_PCA99_Train.csv',\n",
    "    'Activity_PCA99_Validation.csv',\n",
    "    'Activity_PCA99_Test.csv',\n",
    "    'PCA (99% Variance)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary Tables\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all Naive Bayes results\n",
    "nb_results = [\n",
    "    nb_original_val, nb_original_test,\n",
    "    nb_scaled_val, nb_scaled_test,\n",
    "    nb_pca_all_val, nb_pca_all_test,\n",
    "    nb_pca99_val, nb_pca99_test\n",
    "]\n",
    "\n",
    "nb_summary_df = pd.DataFrame(nb_results)\n",
    "nb_summary_df = nb_summary_df[['Dataset', 'Split', 'Accuracy', 'Precision (Macro)', \n",
    "                                 'Precision (Micro)', 'Recall (Macro)', 'Recall (Micro)', \n",
    "                                 'F1-Score (Macro)', 'F1-Score (Micro)']]\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"NAIVE BAYES CLASSIFIER - COMPLETE RESULTS SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(nb_summary_df.to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Classifier Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all Bayes classifier results\n",
    "bayes_results = [\n",
    "    bayes_original_val, bayes_original_test,\n",
    "    bayes_scaled_val, bayes_scaled_test,\n",
    "    bayes_pca_all_val, bayes_pca_all_test,\n",
    "    bayes_pca99_val, bayes_pca99_test\n",
    "]\n",
    "\n",
    "bayes_summary_df = pd.DataFrame(bayes_results)\n",
    "bayes_summary_df = bayes_summary_df[['Dataset', 'Split', 'Accuracy', 'Precision (Macro)', \n",
    "                                       'Precision (Micro)', 'Recall (Macro)', 'Recall (Micro)', \n",
    "                                       'F1-Score (Macro)', 'F1-Score (Micro)']]\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"BAYES CLASSIFIER - COMPLETE RESULTS SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(bayes_summary_df.to_string(index=False))\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparative Analysis: Test Set Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison for test sets only\n",
    "test_comparison = []\n",
    "\n",
    "datasets = ['Original Data', 'Standardized Data', 'PCA (All Components)', 'PCA (99% Variance)']\n",
    "nb_test_results = [nb_original_test, nb_scaled_test, nb_pca_all_test, nb_pca99_test]\n",
    "bayes_test_results = [bayes_original_test, bayes_scaled_test, bayes_pca_all_test, bayes_pca99_test]\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    test_comparison.append({\n",
    "        'Dataset': dataset,\n",
    "        'Classifier': 'Naive Bayes',\n",
    "        'Accuracy': nb_test_results[i]['Accuracy'],\n",
    "        'F1-Score (Macro)': nb_test_results[i]['F1-Score (Macro)'],\n",
    "        'F1-Score (Micro)': nb_test_results[i]['F1-Score (Micro)']\n",
    "    })\n",
    "    test_comparison.append({\n",
    "        'Dataset': dataset,\n",
    "        'Classifier': 'Bayes',\n",
    "        'Accuracy': bayes_test_results[i]['Accuracy'],\n",
    "        'F1-Score (Macro)': bayes_test_results[i]['F1-Score (Macro)'],\n",
    "        'F1-Score (Micro)': bayes_test_results[i]['F1-Score (Micro)']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(test_comparison)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARATIVE ANALYSIS - TEST SET PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Accuracy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar plot comparing accuracies\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Validation set comparison\n",
    "val_data = []\n",
    "for i, dataset in enumerate(datasets):\n",
    "    val_data.append({\n",
    "        'Dataset': dataset,\n",
    "        'Naive Bayes': [nb_original_val, nb_scaled_val, nb_pca_all_val, nb_pca99_val][i]['Accuracy'],\n",
    "        'Bayes': [bayes_original_val, bayes_scaled_val, bayes_pca_all_val, bayes_pca99_val][i]['Accuracy']\n",
    "    })\n",
    "\n",
    "val_df = pd.DataFrame(val_data)\n",
    "val_df.plot(x='Dataset', y=['Naive Bayes', 'Bayes'], kind='bar', ax=ax[0], \n",
    "            color=['#3498db', '#e74c3c'], width=0.7)\n",
    "ax[0].set_title('Validation Set Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "ax[0].set_ylabel('Accuracy', fontsize=12)\n",
    "ax[0].set_xlabel('Dataset Variant', fontsize=12)\n",
    "ax[0].set_ylim([0, 1.0])\n",
    "ax[0].legend(title='Classifier')\n",
    "ax[0].grid(axis='y', alpha=0.3)\n",
    "plt.setp(ax[0].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Test set comparison\n",
    "test_data = []\n",
    "for i, dataset in enumerate(datasets):\n",
    "    test_data.append({\n",
    "        'Dataset': dataset,\n",
    "        'Naive Bayes': nb_test_results[i]['Accuracy'],\n",
    "        'Bayes': bayes_test_results[i]['Accuracy']\n",
    "    })\n",
    "\n",
    "test_df = pd.DataFrame(test_data)\n",
    "test_df.plot(x='Dataset', y=['Naive Bayes', 'Bayes'], kind='bar', ax=ax[1], \n",
    "            color=['#3498db', '#e74c3c'], width=0.7)\n",
    "ax[1].set_title('Test Set Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "ax[1].set_ylabel('Accuracy', fontsize=12)\n",
    "ax[1].set_xlabel('Dataset Variant', fontsize=12)\n",
    "ax[1].set_ylim([0, 1.0])\n",
    "ax[1].legend(title='Classifier')\n",
    "ax[1].grid(axis='y', alpha=0.3)\n",
    "plt.setp(ax[1].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary tables to CSV files\n",
    "nb_summary_df.to_csv('naive_bayes_results.csv', index=False)\n",
    "bayes_summary_df.to_csv('bayes_classifier_results.csv', index=False)\n",
    "comparison_df.to_csv('comparative_analysis.csv', index=False)\n",
    "\n",
    "print(\"Results exported successfully!\")\n",
    "print(\"- naive_bayes_results.csv\")\n",
    "print(\"- bayes_classifier_results.csv\")\n",
    "print(\"- comparative_analysis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Conclusion\n",
    "---\n",
    "\n",
    "This notebook successfully implemented and evaluated:\n",
    "\n",
    "1. **Naive Bayes Classifier** using scikit-learn's GaussianNB\n",
    "2. **Bayes Classifier** implemented from scratch using multivariate Gaussian distributions\n",
    "\n",
    "Both classifiers were tested on four dataset variations:\n",
    "- Original Data (561 features)\n",
    "- Standardized Data (561 features, scaled)\n",
    "- PCA All Components (transformed features)\n",
    "- PCA 99% Variance (reduced features)\n",
    "\n",
    "Key observations:\n",
    "- Both classifiers perform well on the activity detection task\n",
    "- Standardization and PCA transformations may affect performance differently\n",
    "- The Bayes classifier with full covariance modeling may capture more complex relationships\n",
    "- Naive Bayes is computationally more efficient due to the independence assumption\n",
    "\n",
    "All metrics including confusion matrices, accuracy, precision, recall, and F1-scores have been computed for both validation and test sets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}